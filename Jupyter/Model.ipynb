{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to set GPU\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# ClassficationReport\n",
    "# Class to calculate accuracy, precision, recall and f1 score\n",
    "# Object of of this can be consturcted using prediction file, label file and output file \n",
    "\"\"\"\n",
    "\n",
    "class ClassificationReport:\n",
    "    \n",
    "    \n",
    "    def __init__(self, model_res_file, lfile, outfile):\n",
    "        # file containing model predictions\n",
    "        self.model_res_file = model_res_file\n",
    "        \n",
    "        # Unique list of labels (cancer names)\n",
    "        self.lfile = lfile \n",
    "        \n",
    "        # output file\n",
    "        self.outfile = outfile \n",
    "        \n",
    "        # crating array from label file \n",
    "        self.labels = [label.rstrip() for label in open(self.lfile, 'r')] \n",
    "\n",
    "    def parse_results(self):\n",
    "        # read input file and get predicted and groud truth values \n",
    "        df = pd.read_csv(self.model_res_file, sep='\\t', header=None).dropna(axis=1)\n",
    "        ypred = np.array(df.iloc[:, -3])\n",
    "        yreal = np.array(df.iloc[:, -2])\n",
    "\n",
    "        # model accuracy, precicion, recall and f1-scores \n",
    "        acc = accuracy_score(yreal, ypred)\n",
    "        print(\"Accuracy:\", acc)\n",
    "        report = classification_report(yreal, ypred, output_dict=True, zero_division=0)\n",
    "        report = pd.DataFrame(report).transpose()\n",
    "        \n",
    "        print(\"Classification: \",report)\n",
    "        out = os.path.join(self.outfile)\n",
    "\n",
    "        report.to_csv(out)\n",
    "\n",
    "        print('Done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Validate\n",
    "\n",
    "# model_trainer = ModelTrainer(inputfile, testfile, label_file, mname, nfeatures ,resdir)\n",
    "# model, res = model_trainer.run_mlp_model(e=50, test=True)\n",
    "\n",
    "class ModelTrainer:\n",
    "    \n",
    "    def __init__(self, train_data_file, test_data_file, labels_file, mname, nfeatures, outdir):\n",
    "        self.train_data_file = train_data_file\n",
    "        self.test_data_file = test_data_file\n",
    "        self.labels_file = labels_file\n",
    "        self.mname = mname\n",
    "        self.outdir = outdir\n",
    "        self.nfeatures = nfeatures \n",
    "        self.nclasses = self.get_number_of_classes()\n",
    "\n",
    "    def get_number_of_classes(self):\n",
    "        return len([c for c in open(self.labels_file, 'r')])\n",
    "\n",
    "\n",
    "    def run_mlp_model(self, e=20, test=False):\n",
    "        self.predict_txt = self.mname + \"_MLP_predict.txt\"\n",
    "        self.result_log = os.path.join(self.outdir,self.mname + \"_MLP.log\")\n",
    "        \n",
    "        print('==> Loading train and test dataset...')\n",
    "        # get the training data\n",
    "        trainX, testX, trainY, testY = Preprocessor(self.train_data_file, self.test_data_file,\n",
    "                                                    self.labels_file).get_mlp_data()\n",
    "\n",
    "        print('==> Creating  DNN model')\n",
    "        # initialize model\n",
    "        mlp_model = DLmodel(self.nfeatures, self.nclasses).get_mlp_model()\n",
    "\n",
    "        print('==> Training model ...')\n",
    "        dnn_callbacks = tf.keras.callbacks.EarlyStopping(patience=3, monitor='loss')\n",
    "        \n",
    "        # To train mode on 70% the data and validate on 10% and test on 20% data\n",
    "        # Set validation split to 12.5% because training data file contains only 80% data\n",
    "        train_results = mlp_model.fit(trainX, trainY, epochs=e, validation_split=0.125,\n",
    "                                     callbacks=[dnn_callbacks])\n",
    "        \n",
    "        # put model training log into a file \n",
    "        pd.DataFrame(train_results.history).to_csv(self.result_log, sep='\\t')\n",
    "        \n",
    "        if test:\n",
    "            print('==> Testing model ....')\n",
    "            # get the test data\n",
    "            ypred = mlp_model.predict(testX)\n",
    "            self.print_ypred_test_labels(ypred, testY)\n",
    "            print(\"Results dir: \", self.outdir)\n",
    "\n",
    "        return mlp_model, self.get_model_accuracy_and_loss(train_results)\n",
    "    \n",
    "\n",
    "    def get_model_accuracy_and_loss(self, fit_results):\n",
    "        res = {'accuracy': fit_results.history['accuracy'][-1],\n",
    "               'val_accuracy': fit_results.history['val_accuracy'][-1],\n",
    "               'loss': fit_results.history['loss'][-1],\n",
    "               'val_loss': fit_results.history['val_loss'][-1]}\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "    def print_ypred_test_labels(self, ypred, labels):\n",
    "        # save model prediction to file\n",
    "        # alternate column containg softmax and one hot encoded value  \n",
    "        # For example,\n",
    "        # first column softmax value for encoded label 0 \n",
    "        # second column softmax value for encoded label 0\n",
    "        # then encode label of max(softmax) value feteched using argmax\n",
    "        # after that groud truth encoded label\n",
    "        # finally result true / false prediction\n",
    "        \n",
    "        ypred_argmax = np.argmax(ypred, 1)\n",
    "        lab_argmax = np.argmax(labels, 1)\n",
    "        outfile = os.path.join(self.outdir, self.predict_txt)\n",
    "\n",
    "        ylen = ypred.shape[0]\n",
    "        yclasses = ypred.shape[1]\n",
    "\n",
    "        with open(outfile, 'w+') as out:\n",
    "            for rec in range(ylen):\n",
    "                for i in range(yclasses):\n",
    "                    out.write(str(ypred[rec, i]))\n",
    "                    out.write('\\t')\n",
    "                    out.write(str(labels[rec, i]))\n",
    "                    out.write('\\t')\n",
    "\n",
    "                out.write('\\t')\n",
    "                out.write(str(ypred_argmax[rec]))\n",
    "                out.write('\\t')\n",
    "                out.write(str(lab_argmax[rec]))\n",
    "                out.write('\\t')\n",
    "                out.write(str(ypred_argmax[rec] == lab_argmax[rec]))\n",
    "                out.write('\\n')\n",
    "\n",
    "    def print_model_training_progress(self, res):\n",
    "\n",
    "        df = pd.DataFrame(res)\n",
    "        df.to_csv('training_progress.tsv', sep='\\t')\n",
    "        \n",
    "    \n",
    "    # Early Stopping\n",
    "    def get_callbacks(self):\n",
    "        return tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# This class takes train data, test data , and label file \n",
    "# data will be separated into features and labels\n",
    "# It has method to one hot encode labels \n",
    "# \n",
    "\"\"\"\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, inputfile, testfile, label_file):\n",
    "        # file containing training data\n",
    "        self.input_files = inputfile\n",
    "        # file containing test data\n",
    "        self.testfile = testfile\n",
    "        # file containing labels \n",
    "        self.labels_file = label_file\n",
    "\n",
    "    def split_the_data(self):\n",
    "        \n",
    "        data = pd.read_csv(self.input_files, sep=',', index_col=0)\n",
    "        test = pd.read_csv(self.testfile, sep=',', index_col=0 )\n",
    "        print(data.shape)\n",
    "        \n",
    "        # separate features and labels \n",
    "        \n",
    "        # last column contains labels\n",
    "        trainY = data.iloc[:, -1] \n",
    "        testY = test.iloc[:, -1]\n",
    "        \n",
    "        # drop last column to keep features only \n",
    "        trainX = data.iloc[:, :-1] \n",
    "        testX = test.iloc[:, :-1] \n",
    "        \n",
    "        # get one hot encoded labels \n",
    "        trainY = self.get_one_encoded_labels(trainY)\n",
    "        testY = self.get_one_encoded_labels(testY)\n",
    "        \n",
    "        print(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
    "        \n",
    "        return trainX, testX, trainY, testY\n",
    "   \n",
    "    def get_one_encoded_labels(self, y):\n",
    "        y = y.to_numpy().reshape(-1, 1)\n",
    "        labels = np.array([label.strip() for label in open(self.labels_file)]).reshape(-1,1)\n",
    "        ohe = OneHotEncoder()\n",
    "        ohe.fit(labels)\n",
    "        return ohe.transform(y).toarray()\n",
    "\n",
    "    def get_mlp_data(self):\n",
    "        trainX, testX, trainY, testY = self.split_the_data()\n",
    "        trainX = trainX.to_numpy('float64')\n",
    "        testX = testX.to_numpy('float64')\n",
    "\n",
    "        return trainX, testX, trainY, testY\n",
    "    \n",
    "    def get_cnn_data(self):\n",
    "        trainX, testX, trainY, testY =  self.split_the_data()\n",
    "        \n",
    "        trainX = self.reshape_data(trainX)\n",
    "        testX  = self.reshape_data(testX)\n",
    "        \n",
    "    \n",
    "        # Log transformation\n",
    "        #trainX = self.logtranformthe_data(self.reshape_data(trainX), add_to_zeros=1)\n",
    "        #testX  = self.logtranformthe_data(self.reshape_data(testX), add_to_zeros=1)        \n",
    "        \n",
    "        print(\"dataX shape :\", trainX.shape)\n",
    "        return trainX, testX, trainY, testY\n",
    "\n",
    "    def reshape_data(self,x):\n",
    "        print('Reshape data')\n",
    "        x = x.to_numpy('float64')\n",
    "        \n",
    "        # 1 column matrix\n",
    "        x = x.reshape(x.shape[0], x.shape[1], 1)\n",
    "        \n",
    "        # 1 row  matrix\n",
    "        # x = x.reshape(x.shape[0], 1, x.shape[1])\n",
    "\n",
    "        return x\n",
    "\n",
    "    def logtranformthe_data(self, x, base=10, add_to_zeros = 1):\n",
    "        print('log transformed')\n",
    "        x = np.log(x + add_to_zeros) / np.log(base)\n",
    "        return x\n",
    "\n",
    "    def get_test_data(self, isCNN=False):\n",
    "        data  = pd.read_csv(self.input_files, sep='\\t', index_col=0)\n",
    "        dataY = data.iloc[:, -1]  # last column contains labels\n",
    "        dataX = data.iloc[:, :-1]  # droping last columns\n",
    "         \n",
    "        \n",
    "        if(isCNN):\n",
    "            dataX = self.reshape_data(dataX)\n",
    "        else:\n",
    "            dataX = dataX.to_numpy('float64')\n",
    "\n",
    "        return dataX\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from numpy.random import seed\n",
    "\n",
    "# Model\n",
    "\n",
    "class DLmodel:\n",
    "    \n",
    "    \"\"\"\n",
    "    It has only one method and it is a static method which could be used to initialize the cnn model\n",
    "    change code in the create_model method to change the architecture of the model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nfeatures, nclasses):\n",
    "        self.nfeatures = nfeatures\n",
    "        self.nclasses = nclasses\n",
    "  \n",
    "    # method for MLP model\n",
    "    def get_mlp_model(self):\n",
    "        \n",
    "        # delete model if exists in the local or global enviroment  \n",
    "        if ('model' in locals()) or ('model' in globals()):\n",
    "            del(model)\n",
    "            \n",
    "        # set random seed to a constant number before intializing model\n",
    "        tf.random.set_seed(7)\n",
    "        \n",
    "        \"\"\"\n",
    "        DNN model architecture initializing \n",
    "        \"\"\"\n",
    "        print('Creating the model ... ')\n",
    "        \n",
    "        model = tf.keras.models.Sequential()\n",
    "        \n",
    "        # input layer and hidden layers  \n",
    "        model.add(tf.keras.layers.Dense(500, activation='relu', input_shape=(self.nfeatures,)))\n",
    "        model.add(tf.keras.layers.Dense(250, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(125, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(75, activation='relu'))\n",
    "\n",
    "        # output layer\n",
    "        model.add(tf.keras.layers.Dense(self.nclasses, activation='softmax',  name='output_layer'))\n",
    "\n",
    "        # optimizer and learning rate\n",
    "        opt = tf.keras.optimizers.Adam(lr=0.00001)\n",
    "\n",
    "        # Model compile\n",
    "        model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Test_DNN(dname, root_dir, datafile):\n",
    "    \n",
    "    # number of gene used to train model \n",
    "    nfeatures = 12350\n",
    "    \n",
    "    # Create an output directory for DNN model under main output directory   \n",
    "    data_dir = os.path.join(root_dir,dname)\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    # set it as a result dir \n",
    "    resdir = os.path.join(root_dir, dname)\n",
    "    \n",
    "    # create an obeject of DataSplitter class\n",
    "    dsplit = DataSplitter(datafile, data_dir)\n",
    "    \n",
    "    # create Train and Test datasets \n",
    "    dsplit.split_test_train()\n",
    "    inputfile = os.path.join(data_dir, 'TCGA_FPKM_5_log_train.csv')\n",
    "    testfile = os.path.join(data_dir, 'TCGA_FPKM_5_log_test.csv')\n",
    "    \n",
    "    # create Test and Train datasets \n",
    "    dsplit.split_test_data()\n",
    "    \n",
    "    # File containing unique labels \n",
    "    label_file = '/home/n10337547/gpu/extra/Projects/2_GPU_Parallel/TCGA_classes_new.txt'\n",
    "    \n",
    "    # This is hard coded dont change \n",
    "    mname = 'FPKM_5'\n",
    "    sfix = '_MLP'\n",
    "    \n",
    "    # This will train and test model\n",
    "    # Saves predictions,  to file as well as model to h5 file\n",
    "    \n",
    "    model_trainer = ModelTrainer(inputfile, testfile, label_file, mname, nfeatures ,resdir)\n",
    "    model, res = model_trainer.run_mlp_model(e=50, test=True)\n",
    "    model_file = os.path.join(resdir, mname + sfix +'.h5')\n",
    "    model.save(model_file)\n",
    "\n",
    "    model_res_file = os.path.join(resdir, mname + sfix + \"_predict.txt\")\n",
    "    report_file = os.path.join(resdir, mname + sfix + \"_confusion_matrix.txt\")\n",
    "    ClassificationReport(model_res_file, label_file, report_file).parse_results()\n",
    "    \n",
    "    print(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data file containing TCGA gene expression data \n",
    "# Rows = samples \n",
    "# columns = genes and last column = label (cancer name)\n",
    "# Values = log tranformed FPKM values \n",
    "# input data file \n",
    "dfile = '/path/to/input_data_file.csv'\n",
    "# output directory  \n",
    "outdir = '/path/to/folder'\n",
    "\n",
    "# To create 10 models, we will run code 10 times \n",
    "i = 1\n",
    "while i < 11:\n",
    "    # output names DNN_1, DNN_2 ... DNN_10 \n",
    "    dname = \"DNN_\"+str(i)\n",
    "    print(dname)\n",
    "    \n",
    "    # Run model \n",
    "    train_Test_DNN(dname, outdir, dfile)\n",
    "    i += 1\n",
    "\n",
    "\n",
    "print('--- DONE ---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
